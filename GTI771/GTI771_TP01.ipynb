{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlasmaLotus/EmptyProject/blob/main/GTI771/GTI771_TP01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5hXSJAP5CTP"
      },
      "source": [
        "# GTI771 - Apprentissage machine avancé\n",
        "## Département du génie logiciel et des technologies de l’information\n",
        "\n",
        "\n",
        "\n",
        "## Laboratoire 1 - Analyse et préparation des données et extraction de primitives\n",
        "#### <font color=black> Version 6.0 janvier 2022 </font>\n",
        "#### <font color=white> Version 4.0 mai 2020 </font>\n",
        "#### <font color=white> Version 5.0 janvier 2021 </font>\n",
        "\n",
        "##### Prof. Alessandro L. Koerich"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jMTsIjF5CTR"
      },
      "source": [
        "| NOMS                  | CODE PERMANENT                                   |\n",
        "|-----------------------|--------------------------------------------------|\n",
        "| Lee-Stenio Nazer      | Nazl14039501                                     |\n",
        "| Étudiant2             | Code2                                            |\n",
        "| Étudiant3             | Code3                                            |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxXfak6Y5CTR"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Ce premier laboratoire porte sur la définition et l’extraction de primitives sur des visages. Le problème de classification qui vous est présenté est le problème [Facial Expression Recognition (FER)](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data), dont le but est de classer des visages dans sept catégories. En vous basant sur les concepts vus en classe, vous devez définir des primitives que vous jugez pertinentes à extraire sur ces types d’images et effectuer l’extraction de celles-ci sur l’ensemble de données fournies avec cet énoncé.\n",
        "\n",
        "Veuillez noter que les images qui vous sont fournies ne sont pas nécessairement très faciles à travailler. Plusieurs images comportent du bruit, des artéfacts ou des éléments non pertinents. Le défi de ce laboratoire repose sur cette difficulté qui est chose courante dans des problèmes d’apprentissage machine moderne.\n",
        "\n",
        "Voici, en exemple, des images de visages se retrouvant dans l’ensemble de données:\n",
        "\n",
        "![Exemples de FER](https://miro.medium.com/max/2420/1*nXqJ4lMiBRp4Ilm3bpRxuA.png)\n",
        "\n",
        "\n",
        "L’évaluation de ce laboratoire sera basée sur la qualité des primitives proposées, les réponses aux questions dans cette notebook ainsi que l'organisation de votre code source <font color=black> (SVP, n'oubliez pas de mettre des commentaires dans le code!)</font>. Le pouvoir discriminant, c’est-à-dire la capacité des primitives à bien séparer les exemples des classes dans l’espace des primitives, sera également évalué."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "GrHNDTcH5CTS"
      },
      "source": [
        "# Partie 1: Imports (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki67Nhzm5CTS"
      },
      "source": [
        "### 1a: Import de bibliotèques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-U5kDKZ5CTS"
      },
      "source": [
        "### À faire:\n",
        "1. Ajouter toutes les bibliothèques que vous avez utilisées pour compléter ce notebook dans une cellule avec une petite description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "GmY_4nCm5CTS",
        "outputId": "46e8ec6c-3a6f-45e5-c7ec-beebbc1c4d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9d1d3b77-7a36-4856-b9e5-8d7b55bcb08d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9d1d3b77-7a36-4856-b9e5-8d7b55bcb08d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fer2013.csv to fer2013.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np  # package for scientific computing with Python.\n",
        "import matplotlib.pyplot as plt # 2D plotting library\n",
        "print('hello')\n",
        "# Votre code ici\n",
        "import pandas as pd\n",
        "import io\n",
        "from google.colab import files\n",
        "#data_to_load = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBjCRKeq5CTT"
      },
      "source": [
        "#### Fonctions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8diEq8e45CTT"
      },
      "outputs": [],
      "source": [
        "def fa():\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRiCVEaF5CTT"
      },
      "source": [
        "## Partie 2: Analyse exploratoire des données (10 points)\n",
        "\n",
        "On va commencer en regardant les données.\n",
        "\n",
        "Pour ce lab, nous allons utiliser le dataset FER.\n",
        "\n",
        "Le dataset est disponible dans https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data, et il continent presque 35,000 visages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QrlLOPg5CTT"
      },
      "source": [
        "## 2a: Charger le fichier de données (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgTo2sM35CTU",
        "outputId": "836606e5-8da5-4dc7-a09b-294fc6ed61eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28709, 2304) (3589, 2304) (3589, 2304)\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "#df = pd.read_csv(io.BytesIO(data_to_load['fer2013.csv']))\n",
        "\n",
        "ferData = np.loadtxt( 'fer2013.csv', delimiter=',', dtype=str )\n",
        " \n",
        "# Training set\n",
        "#nbExamplesTraining:28709\n",
        "#imageSize:2304 = 48x48\n",
        "Xtrain = np.ones( (28709,2304), float )\n",
        "\n",
        "for i in range( 1, 28710 ):\n",
        "    Xtrain[i-1] = ferData[i,1].split(\" \")\n",
        "\n",
        "ytrain = ferData[1:28710,0].astype( int )\n",
        "\n",
        "# Validation set\n",
        "Xval = np.ones( (3589,2304), float )\n",
        "\n",
        "for i in range( 28710, 32299 ):\n",
        "    Xval[i-28710] = ferData[i,1].split(\" \")\n",
        "\n",
        "yval = ferData[28710:32299,0].astype( int )\n",
        "\n",
        "# Test set\n",
        "Xtest = np.ones( (3589,2304), float )\n",
        "\n",
        "for i in range( 32299, 35888 ):\n",
        "    Xtest[i-32299] = ferData[i,1].split(\" \")\n",
        "\n",
        "ytest = ferData[32299:,0].astype( int )\n",
        "\n",
        "print(Xtrain.shape, Xval.shape, Xtest.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8zJDvwu5CTU",
        "outputId": "87d59604-da0e-42e3-ae49-e7239e58e345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28709, 1, 48, 48) (3589, 1, 48, 48) (3589, 1, 48, 48)\n"
          ]
        }
      ],
      "source": [
        "# reshape to be [samples][channels][width][height]\n",
        "Xtrain = Xtrain.reshape( Xtrain.shape[0], 1, 48, 48 ).astype('uint8')\n",
        "Xtest  = Xtest.reshape( Xtest.shape[0], 1, 48, 48 ).astype('uint8')\n",
        "Xval   = Xval.reshape( Xval.shape[0], 1, 48, 48 ).astype('uint8')\n",
        "\n",
        "print( Xtrain.shape, Xval.shape, Xtest.shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqRbkI4e5CTU"
      },
      "source": [
        "## 2b: Pre-traitement et visualisation des visages (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv1EKsf05CTU"
      },
      "source": [
        "Vous pouvez visualiser les images en utilisant `plt.imshow`.\n",
        "\n",
        "Il y a différents types de prétraitement que nous pouvons appliquer à des images dans les ensembles de données pour réduire la variabilité, réduire des bruits, etc.\n",
        "\n",
        "### À faire:\n",
        "1. Pensez-vous qu’est nécessaire un prétraitement des images? Si oui, vous pouvez choisir différents algorithmes de prétraitement dans [scikit-image](https://scikit-image.org/docs/stable/api/api.html). Il y a aussi autres types de prétraitement plus généraux dans [scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing).\n",
        "2. Expliquer et justifier les prétraitements choisis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK-k0pwH5CTU"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNhD7DKE5CTU",
        "outputId": "8c1215b0-7797-4046-f8b6-71ed46bd2fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11 10 12 ... 10 10 10]\n",
            " [10 11 13 ... 10 10 11]\n",
            " [11 10 12 ... 11 11 10]\n",
            " ...\n",
            " [10  9 12 ... 11 10 12]\n",
            " [10  9 12 ... 11 13 14]\n",
            " [11  9 12 ... 12 14 16]]\n",
            "66.51095583863209\n",
            "[[ 4.    5.52  6.68 ...  6.12  4.96  3.72]\n",
            " [ 5.28  7.28  8.84 ...  8.56  7.04  5.04]\n",
            " [ 6.64  9.12 11.08 ... 10.56  8.64  6.24]\n",
            " ...\n",
            " [ 6.32  8.72 10.72 ... 16.68 11.6   7.12]\n",
            " [ 5.04  6.92  8.56 ... 11.24  7.92  5.64]\n",
            " [ 3.76  5.16  6.44 ...  7.52  5.92  4.52]]\n",
            "(48, 48)\n",
            "[[-0.4278647   0.28900547 -1.87969814 ...  1.14183753 -1.36564443\n",
            "  -0.947846  ]\n",
            " [ 1.03680398 -1.19713322  0.11316456 ... -0.57558432 -0.46828369\n",
            "   0.67640397]\n",
            " [-0.01527279  0.54757267 -0.7123211  ... -1.25242809 -1.16245888\n",
            "  -1.06251496]\n",
            " ...\n",
            " [-1.46379119  1.72469425 -1.41221073 ... -0.248619   -1.06902138\n",
            "  -0.74563778]\n",
            " [ 0.22413216  0.36675846  0.39647109 ... -0.08800813  0.98422508\n",
            "  -1.63293369]\n",
            " [-0.92198606 -0.16504954  0.17598891 ... -0.0239959   0.03374349\n",
            "  -0.43889629]]\n",
            "[[-1.         -1.          1.         ...  1.          1.\n",
            "   1.        ]\n",
            " [-1.         -1.         -1.         ...  1.          1.\n",
            "   1.        ]\n",
            " [ 1.          1.          1.         ...  1.          1.\n",
            "   1.        ]\n",
            " ...\n",
            " [ 1.          1.          1.         ... -1.         -1.\n",
            "  -1.        ]\n",
            " [ 1.          1.          1.         ... -1.         -1.\n",
            "   0.06844163]\n",
            " [-1.          1.          1.         ... -1.          0.45834732\n",
            "   0.24737431]]\n"
          ]
        }
      ],
      "source": [
        "# Votre code de pre-processing\n",
        "# Normalize pictures\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "#scaler_Xtrain = preprocessing.StandardScaler().fit(Xtrain)\n",
        "#scaler_Xtest = preprocessing.StandardScaler().fit(Xtest)\n",
        "#scaler_Xval = preprocessing.StandardScaler().fit(Xval)\n",
        "\n",
        "#Xtrain_scaled = scaler_Xtrain.transform(Xtrain)\n",
        "#Xtrain_scaled = scaler_Xtrain.transform(Xtrain)\n",
        "#Xtrain_scaled = scaler_Xtrain.transform(Xtrain)\n",
        "\n",
        "\"\"\"\n",
        "#!pip install scipy\n",
        "from skimage import color, data, restoration\n",
        "from scipy.signal import convolve2d as conv2\n",
        "\n",
        "#np.fft.\n",
        "# Weiner filter\n",
        "\n",
        "print(Xtrain[28700][0])\n",
        "psf = np.ones((5, 5)) / 25\n",
        "#psf = np.ones((48, 48)) /(25 *255) #* 255 / 25\n",
        "image = conv2(Xtrain[28700][0], psf, 'same')\n",
        "print(Xtrain[28700][0].std())\n",
        "print(image)\n",
        "print(image.shape)\n",
        "print(np.random.standard_normal(image.shape))\n",
        "image += 0.1 * image.std() * np.random.standard_normal(image.shape,)\n",
        "deconvolved = restoration.wiener(image, psf, 0.01)\n",
        "#print(deconvolved)\n",
        "\n",
        "#print(Xtrain.argmax(axis=3))\n",
        "#print(psf)\n",
        "#print(Xtrain[28700][0])# image array [48][48]\n",
        "print(deconvolved)\n",
        "\"\"\"\n",
        "\n",
        "#https://stackoverflow.com/questions/41019222/deblur-an-image-using-scikit-image\n",
        "#https://stackoverflow.com/questions/35192550/wiener-filter-for-image-deblur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_FayaPH5CTU"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dcYK8xa5CTV"
      },
      "outputs": [],
      "source": [
        "# Votre explication/justification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OLXnUhR5CTV"
      },
      "source": [
        "## 2c: Statistiques des données (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Uht7jD5CTV"
      },
      "source": [
        "### À faire:\n",
        "1. Calculer quelques statistiques descriptives (moyenne/écart-type/min/max/etc ) que vous jugez importantes/pertinents sur les entrées (pixels/regions l'images/etc) et sorties (classe/sexe/ethinie/age etc.)\n",
        "\n",
        "2. Faire une analyse des résultats et présenter vos conclusions sur ces statistiques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvJmD8A5CTV"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "nTH3ork55CTV",
        "outputId": "9b26d0d9-f4e5-4e03-ba7e-845e252b536e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.  0.  1.  2.  3.  4.  5.  6.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Votre code ici\n",
        "\n",
        "# Code exemple:\n",
        "# Histogramme des étiquettes (classes)\n",
        "hist, _ = np.histogram(ytrain, density=False, bins=7, range=(0, 7))\n",
        "\n",
        "# Code exemple:\n",
        "# Code exemple\n",
        "# Plot du histogramme\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.hist(ytrain, bins = [0,1,2,3,4,5,6,7]) \n",
        "\n",
        "ax.set_xticklabels([])\n",
        "#(0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
        "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "mticks = ax.get_xticks()\n",
        "mticks = mticks[0:-2]\n",
        "#print(mticks)\n",
        "ax.set_xticks((mticks[:-1]+mticks[1:])/2+1, minor=True)\n",
        "ax.tick_params(axis='x', which='minor', length=0)\n",
        "ax.set_xticklabels(labels, minor=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hNMC6yG5CTV"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQPqEgiZ5CTV"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAA96KKL5CTV"
      },
      "source": [
        "## 2d: Créer et évaluer un modèle \"template matching\" (3 points)\n",
        "\n",
        "Un deuxièmme modèle simple, mais naturel est une où nous calculons un modèle (prototype/template) moyen (moyenne des valeurs de pixels) pour chaque classe (sur les données d'apprentissage) et nous utilisons ces modèles (templates) pour faire des prédictions sur des nouvelles données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1GNCMUL5CTV"
      },
      "source": [
        "### À faire:\n",
        "1. Créer un template/prototype pour chaque classe\n",
        "2. Faire un plot de chaque template <font color=black> (visage moyen pour chaque classe) </font>\n",
        "3. Classifier par \"template matching - plus proche prototype\", tous les exemples (ensembles d'apprentissage, validation et test) et reporter les résultats:<br>\n",
        " 3a. taux de classification correct sur les trois (3) ensembles de données (sous la forme d'un tableau)<br>\n",
        " 3b. matrice de confusion pour les résultas sur l'ensemble de test (matrice 7 x 7 - etiquette x prédictions) \n",
        "4. Faire une analyse des résultats et présenter vos conclusions sur ce modèle (performe bien/mal, pour quoi?, faiblasses/fortesses, possibles améliorations, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu9q6ZKS5CTW"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNzPXUZ35CTW"
      },
      "outputs": [],
      "source": [
        "# Votre code de création des modèles et template matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzqUE1vX5CTW"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrdIL33p5CTW"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuLsYRDK5CTW"
      },
      "source": [
        "# Partie 3: Extraction de primitives (20 points)\n",
        "\n",
        "Vous devez faire une recherche bibliographique pour trouver quelles sont les primitives qui sont plus souvent utilisées pour la reconnaissance des expressions faciales. Voici quelques sources et mot-clés pour guider votre recherche:\n",
        "\n",
        "- http://www.inf.ufpr.br/lesoliveira/download/ESWA2013.pdf\n",
        "- https://doi.org/10.1016/j.patrec.2015.06.007 \n",
        "- https://doi.org/10.1109/FG.2011.5771374\n",
        "- https://www.hindawi.com/journals/ijbi/2015/267807/\n",
        "\n",
        "\n",
        "Mots-clés: facial expression recognition, facial emotion recognition.\n",
        "\n",
        "Bibliothèques Python pour l'extraction de primitives :\n",
        "* [Scikit-image](https://scikit-image.org/docs/dev/)\n",
        "* [OpenCV](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)\n",
        "* [Scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html)\n",
        "\n",
        "Primitives candidates : \n",
        "\n",
        "* LBP, LPQ, Gabor filters, SIFT, SURF, HOG, GLCM, Haralick Moments, etc.\n",
        "\n",
        "\n",
        "#### <font color=red> Attention! Les équipes doivent utiliser des primitives différentes! </font>\n",
        "- Chaque équipe doit poster un message dans le \"forum TP1\" avec sa choix\n",
        "- Premier arrivé premier servi!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5uu86zm5CTW"
      },
      "source": [
        "## 3a: Extraire des primitives (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbAi-1vR5CTW"
      },
      "source": [
        "### À faire:\n",
        "1. Choisir une primitive et extraire ces primitives des visages (ensembles d'apprentissage, validation et test).\n",
        "Un groupe de primitives peut être 16 filtres Gabor, un ensemble de Haar-like features ou bien les différentes features de Haralick Texture Features (p. ex. contraste, homogénéité, etc.). Vous devez en sélectionner qu'une primitive.\n",
        "\n",
        "Attention! Le résultat des algorithmes d'extraction de primitives doit être des vecteurs de primitives. Il y a des algorithmes qui nécessitent d'une étape additionnelle comme calculer les histogrammes de primitives (p. ex. LBP, LPQ, SIFT, etc.)\n",
        "\n",
        "2. <font color=black>Sauvegarder vos vecteurs de primitives dans des fichiers '.csv'. Utiliser la même structure du fichier FER3013, où nous avons dans un seul fichier, séquentiellement, les trois partitions: apprentissage (0-28709), validation (28710-32299) et test (32299, 35888). Nommer vos fichiers de primitive en référence à la primitive utilisée. Ex.: lbp83_fer2013.csv </font> \n",
        "\n",
        "3. Justifier votre choix de primitives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EINusbWz5CTW"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10010Dzk5CTX"
      },
      "outputs": [],
      "source": [
        "# Votre code ici\n",
        "#Using GLCM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II3wFhpR5CTX"
      },
      "source": [
        "#### Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GvAfSvI5CTX"
      },
      "source": [
        "Vos reponses ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX1ZVycu5CTX"
      },
      "source": [
        "## 3b: Analyse visuelle des primitives (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4Nwp4qC5CTX"
      },
      "source": [
        "##### À faire:\n",
        "1. Choisir quelques paires de primitives (deux dimensions de votre vecteur de primitives) et tracer un graphique 2D (données d'apprentissage) pour visualiser si ces primitives sont capables de bien séparer les sept (7) classes.\n",
        "2. Faire une analyse des résultats de visualisation et présenter vos conclusions sur ces primitives/paires choisies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsK5wgjQ5CTY"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTP-JTn25CTY"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqSgU37U5CTY"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cua5M3o_5CTY"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wJmZoIP5CTY"
      },
      "source": [
        "# Partie 4: Construction des modèle (20 points)\n",
        "\n",
        "Nous sommes maintenant prêtes à entraîner un premier modèle avec les primitives extraites dans la Partie 3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frQxKgve5CTY"
      },
      "source": [
        "## 4a: Entraîner des modèles (10 points)\n",
        "\n",
        "1) Vous devez choisir un des algorithmes d'apprentissage suivants (déjà vues en LOG635) et l'entraîner sur les données d'apprentissage:<br>\n",
        "* [Decision trees](https://scikit-learn.org/stable/modules/tree.html) de scikit-learn. <br>\n",
        "* [k-NN](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification) de scikit-learn. <br>\n",
        "* [Naïve Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) de scikit-learn. \n",
        "\n",
        "2) Un fois choisi l'algorithme d'apprentissage, vous devez entraîner deux modèles:<br>\n",
        "- Modèle baseline: Entraîner sur les pixels bruts (Partie 2a) ou sur les images de visage prétraites (Partie 2b). Dans ce cas, vous devez \"vectoriser\" vos images 48x48 pour les transformer dans des vecteurs 2304-dimensional. \n",
        "- Modèle \"primitive\": Entraîner sur les vecteurs de primitive générés dans la Partie 3.\n",
        "\n",
        "Pour ces premiers modèles entrînes, vous n'êtes pas censé de les régler (optimisation des hyperparamètres). Vous allez faire ce réglage des hyperparamètres dans la Partie (4b).\n",
        "\n",
        "Après l'entraînement, vous devez faire des prédictions sur les données de validation et test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeHDdRaA5CTY"
      },
      "source": [
        "### À faire:\n",
        "1. Expliquer/jurtifier pourquoi vous avez choisi cet algorithme d'apprentissage.\n",
        "2. Entraîner les modèles avec l'algorithme d'apprentissage choisi\n",
        "3. Classifier tous les exemples (ensembles d'apprentissage, validation et test) avec les deux modèles et reporter les résultats:<br>\n",
        " 3a. taux de classification correct sur les trois (3) ensembles de données<br>\n",
        " 3b. matrice de confusion pour l'ensemble de test\n",
        "4. Sauvagarder votre modèle dans un fichier. Regarder [model persistence](https://scikitlearn.org/stable/modules/model_persistence.html?highlight=persistence)\n",
        "5. Mettre les résultats dans un tableau et les comparer avec le modèle \"template matching\"\n",
        "5. Faire une analyse des résultats et présenter vos conclusions sur la performance des modèles.\n",
        "6. Concernant vos primitives, sont-elles discriminantes? Oui/non/pour quoi?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj7_2eQl5CTY"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNEhbLjm5CTZ"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IJLXqyj5CTZ"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mYTA-V25CTZ"
      },
      "source": [
        "#### Vos résultats ici:\n",
        "\n",
        "##### Exemple:\n",
        "\n",
        "Taux de classification correcte\n",
        "\n",
        "| Ensemble | modele pixel |  modèle TM  | modèle Haralick |                                 \n",
        "|----------|--------------|-------------|-----------------|\n",
        "| App      | 99,67%       |             |                 |                   \n",
        "| Val      | 89,77%       |             |                 |                             \n",
        "| Test     | 77,99%       |             |                 |        \n",
        "\n",
        "Q1: Nous avons choisi l'algorithme d'arbre de décision parce que...\n",
        "\n",
        "Q2: Les taux de classification indiquent que le modèle...\n",
        "\n",
        "Q3: La performance avec le primitive..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXBCLdRI5CTZ"
      },
      "source": [
        "## 4b: Réglage du modèle (10 points)\n",
        "\n",
        "Probablement le modèle \"primitive\" a outperformé les modèle baseline et TM, mais est-ce qu'on peut mieux performer? <br>\n",
        "\n",
        "Est-ce qu'on peut faire un \"fine-tuning\" de vos modèles?<br>\n",
        "\n",
        "Quels sont les paramètres que vous pouvez ajuster dans l'algorithme d'apprentissafe pour essayer d'améliorer la performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjLAs5Fm5CTZ"
      },
      "source": [
        "##### À faire:\n",
        "1. Lister les principaux hyperparamètres de votre algorithme d'apprentissage qui peuvent aider à améliorer sa performance.\n",
        "2. Choisir quelques paramètres (au moins 2) et tourner un \"grid search\" sur l'ensemble de validation pour trouver les meilleures valeurs pour ces paramètres.\n",
        "3. Une fois que vous avez trouvé les meilleurs modèles (primitive, baseline et TM), utiliser ces modèles pour reclassifier tous les exemples (apprentissage, validation, test) et reporter les résultats (comme dans la Partie 4a):<br>\n",
        " 3a. taux de classification correct sur les 3 ensembles<br>\n",
        " 3b. matrice de confusion pour l'ensemble de test\n",
        "4. Sauvagarder votre modèle dans un fichier. Regarder [model persistence](https://scikitlearn.org/stable/modules/model_persistence.html?highlight=persistence)\n",
        "5. Explique pourquoi vous avez choisi ces hyperparamètres pour les optimiser.\n",
        "6. Faire une analyse des résultats et présenter vos conclusions sur les modèles optimises (comparer avec ceux obténus dans la Partie 4a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywi8t0WM5CTZ"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxostBwp5CTZ"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxNdFXeE5CTa"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOhWstnR5CTa"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aykGaBcD5CTa"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laISewZg5CTa"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici: voir Partie 4a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2R7s6im5CTa"
      },
      "source": [
        "# Partie 5: Réduction de la dimensionnalité (20 points)\n",
        "\n",
        "Bibliotèques Python pour la reduction de la dimensionnalité : \n",
        "\n",
        "* [Transformation algorithms](https://scikit-learn.org/stable/modules/decomposition.html#decompositions)\n",
        "* [Feature selection algorithms](https://scikit-learn.org/stable/modules/feature_selection.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM0fPYWf5CTa"
      },
      "source": [
        "## 5a: Réduction de la dimensionnalité (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srqwP0745CTa"
      },
      "source": [
        "### À faire:\n",
        "1. Choisir un algorithme de transformation de primitives et un algorithme de sélection des primitives pour réduire la dimensionnalité de vos vecteurs de primitives (baseline et primitive).\n",
        "2. Choisir les deux meilleures primitives et ploter un graphique primitive $pc_1$ (axe x) X primitive $pc_2$ (axe y) X classe (colour) pour visualiser si juste ces deux primitives sont capables de bien séparer les sept (7) classes.\n",
        "3. Faire une analyse des résultats et présenter vos conclusions sur ces primitives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3EdOLns5CTb"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JziKCrb5CTb"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj2FB8g85CTb"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xU1vPR55CTb"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp3Qhi0t5CTb"
      },
      "source": [
        "## 5b: Rentraîner les modèles de la Partie 4a (5 points)\n",
        "\n",
        "Vous devez rentraîner les modèles (baseline et primitive) avec les vecteurs reduits sur les données d'apprentissage.<br>\n",
        "\n",
        "Après l'rentraînement, vous devez faire des prédictions sur les données de test (reduit par la même transformation/selection).<br>\n",
        "\n",
        "Présenter les résults (comme dans les Parties 4a et 4b) et comparer avec ceux des Parties 4a et 4b "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOvSrP4J5CTb"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVNknkpt5CTb"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUm4eSQ_5CTc"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki1MdrFZ5CTc"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Dp0zPM5CTc"
      },
      "source": [
        "## 5c: Réglage du modèle (5 points)\n",
        "\n",
        "Vous devez reoptimiser vos nouveaux modèles et les entraîner sur les données d'apprentissage (comme dans la Partie 4b).<br>\n",
        "\n",
        "Après l'entraînement, vous devez faire des prédictions sur les données de test.<br>\n",
        "\n",
        "Présenter les résults (comme dans les Parties 4a, 4b et 5a) et comparer avec ceux des Parties 4a, 4b et 5a "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48POYF9h5CTc"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh3MnimM5CTc"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujLHgpdV5CTc"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cN54rbo5CTc"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpq5mo0C5CTd"
      },
      "source": [
        "# Partie 6: Analyse qualitative (5 points)\n",
        "\n",
        "Choisir dans l'ensemble de test, quelques exemples de visages bien et mal classifié par le modèle le plus performant et montrer ces exemples avec la vraie étiquette et la prédiction faite par votre meilleur modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tulOa1q75CTd"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JMuDbat5CTd"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpVMChOV5CTd"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anF3jKSk5CTd"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njstfcBS5CTd"
      },
      "source": [
        "\n",
        "# Partie 7: Cross-Dataset Challenge (10 points)\n",
        "\n",
        "Vous devez tester la capacité de généralisation de votre modèle \"primitive\" (déjà entraîné et testé sur FER) sur un autre ensemble de données.\n",
        "    \n",
        "L’ensemble de données en question c'est le Jaffe dataset qui contient 213 images de sept (7) expressions faciales pose par 10 modèles féminines japonaises. Chaque image a été étiquetée dans six (6) adjectives d’émotion par 60 sujets japonaises. \n",
        "\n",
        "L’ensemble de données originel contient des images 256x256 en niveau de gris, dans le format .tiff, sans compression.\n",
        "\n",
        "Vous avez les données redimensionnées à 48x48 pixels (`jaffe_48x48.csv`), aussi que les données dans la dimension originelle (`jaffe_256x256.csv`). \n",
        "    \n",
        "* Angry (30 exemples):   Y_test[0:30]   = 0 \n",
        "* Disgust (29 exemples): Y_test[30:59]  = 1\n",
        "* Fear (32 exemples): Y_test[59:91]  = 2 \n",
        "* Happy (31 exemples): Y_test[91:122] = 3 \n",
        "* Neutral (30 exemples): Y_test[122:152]= 6 \n",
        "* Surprise (31 exemples): Y_test[152:183]= 5 \n",
        "* Sad (30 exemples): Y_test[183:213]= 4 \n",
        "\n",
        "Attention! Est-ce que le rapport émotion x étiquette numérique est le même que FER?\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiDXiuHF5CTd"
      },
      "source": [
        "## 7a: Charger le fichier de données de test (2 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSXZtXh_5CTd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Read from file\n",
        "X_test_jaffe = np.loadtxt('jaffe_48x48.csv', delimiter=',', dtype=int)\n",
        "#X_test_jaffe = np.loadtxt('jaffe_256x256.csv', delimiter=',', dtype=int)\n",
        "y_test_jaffe = np.loadtxt('jaffe_labels.csv', delimiter=',', dtype=int)\n",
        "\n",
        "# reshape to be [samples][pixels][width][height]\n",
        "X_test_jaffe  = X_test_jaffe.reshape(X_test_jaffe.shape[0], 1, 48, 48).astype('uint8')\n",
        "#X_test_jaffe  = X_test_jaffe.reshape(X_test_jaffe.shape[0], 1, 256, 256).astype('uint8')\n",
        "\n",
        "X_test_jaffe.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVVmZXUf5CTe"
      },
      "source": [
        "\n",
        "## 7b: Visualisation des visages de Jaffe (8 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oVg8Abe5CTe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def getLabel(id):\n",
        "    return ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'][id]\n",
        "plt.figure(figsize=(10, 10))\n",
        "j = 0\n",
        "for i in [5, 40, 70, 105, 135, 171, 191]:\n",
        "    plt.subplot(330 + 1 + j)\n",
        "    plt.imshow(np.squeeze(X_test_jaffe[i], axis=0),cmap=plt.get_cmap('gray'))\n",
        "    plt.gca().get_xaxis().set_ticks([])\n",
        "    plt.gca().get_yaxis().set_ticks([])\n",
        "    plt.ylabel('Label = %s' % getLabel(y_test_jaffe[i]), fontsize = 12)\n",
        "    j = j + 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EnIdvJZ5CTe"
      },
      "source": [
        "\n",
        "### À faire:\n",
        "1. Apliquer les mêmes prétraitemens sur les images de Jaffe\n",
        "2. Extraire les mêmes primitives des images de Jaffe\n",
        "3. Avoir les mêmes étiquettes (chiffre x émotion) de FER\n",
        "1. Tester vos modèles sur les 213 images de Jaffe dataset.\n",
        "2. Faire une analyse des résultats et présenter vos conclusions sur la capacité de generalisation de votre modèle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVw3fEF95CTe"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9sTwpbp5CTe"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJot05z85CTe"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pHRpDrd5CTe"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O9B2uap5CTe"
      },
      "source": [
        "## Partie Finale : Conclusion (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m89zmoeu5CTf"
      },
      "source": [
        "### À faire:\n",
        "1. Résumer et comparer les principaux résultats .\n",
        "2. Faire une analyse des résultats obtenus et présenter vos conclusions sur les différents modèles que vous avez entraînés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKDGEQTZ5CTf"
      },
      "source": [
        "#### Résultats et résponses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0WfP2dh5CTf"
      },
      "outputs": [],
      "source": [
        "# Vos résultats ici:"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}